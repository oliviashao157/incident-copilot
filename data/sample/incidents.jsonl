{"id": "sample-001", "title": "High API latency on checkout service", "description": "We're seeing elevated P99 latency on the checkout service. Response times have increased from 200ms baseline to 3500ms. Started around 14:00 UTC. Approximately 5000 users are affected. Investigation shows database connection pool exhaustion.", "category": "latency", "severity": "high", "source": "synthetic", "created_at": "2024-01-15T14:00:00", "resolved_at": "2024-01-15T15:30:00", "resolution": "Identified slow database queries and added missing index on orders table. Deployed fix and latency returned to normal within 15 minutes.", "labels": ["latency", "high", "database"], "metadata": {"generator": "sample"}}
{"id": "sample-002", "title": "Payment service completely down - 503 errors", "description": "CRITICAL: Payment service is returning 503 errors for all requests. Impact: all customers affected, no checkouts possible. Started at 09:15 UTC. All 8 pods are showing unhealthy status. Error logs show: Connection refused to payment-processor.", "category": "outage", "severity": "critical", "source": "synthetic", "created_at": "2024-01-14T09:15:00", "resolved_at": "2024-01-14T09:45:00", "resolution": "Root cause was network policy misconfiguration after recent deployment. Reverted network policy and service recovered. Implementing additional health checks.", "labels": ["outage", "critical", "payment"], "metadata": {"generator": "sample"}}
{"id": "sample-003", "title": "Failed deployment of user-service v2.5.0", "description": "Deployment of user-service version 2.5.0 failed during rollout. 60% of pods stuck in CrashLoopBackOff. Error: missing DATABASE_URL environment variable. Previous version: 2.4.3.", "category": "deployment", "severity": "high", "source": "synthetic", "created_at": "2024-01-13T16:00:00", "resolved_at": "2024-01-13T16:30:00", "resolution": "Rolled back to 2.4.3. Root cause: missing environment variable in new deployment manifest. Fixed in 2.5.1.", "labels": ["deployment", "high"], "metadata": {"generator": "sample"}}
{"id": "sample-004", "title": "Misconfigured rate limiter causing customer impact", "description": "Incorrect rate limiter configuration deployed to API gateway. Affected environments: production. Error: legitimate requests being rejected with 429. Configuration change made at 11:00 UTC.", "category": "config", "severity": "medium", "source": "synthetic", "created_at": "2024-01-12T11:00:00", "resolved_at": "2024-01-12T11:45:00", "resolution": "Reverted configuration to previous known-good state. Updated validation to prevent similar issues. Added config review requirement.", "labels": ["config", "medium", "rate-limit"], "metadata": {"generator": "sample"}}
{"id": "sample-005", "title": "OOM kills on order-processor pods", "description": "Order-processor pods being OOM killed. Memory usage spiked to 4.5GB against limit of 4GB. 6 pods affected. Cause appears to be memory leak in order validation code introduced in last release.", "category": "capacity", "severity": "high", "source": "synthetic", "created_at": "2024-01-11T08:00:00", "resolved_at": "2024-01-11T10:00:00", "resolution": "Increased memory limits to 6GB as temporary fix. Identified memory leak in OrderValidator class and deployed fix. Memory usage now stable at 2GB.", "labels": ["capacity", "high", "memory"], "metadata": {"generator": "sample"}}
{"id": "sample-006", "title": "Database replication lag causing stale reads", "description": "Replication lag on postgres-replica-1 reached 120 seconds. Read queries returning stale data. Primary at 85% CPU. 15 slow queries identified in analytics dashboard.", "category": "data", "severity": "medium", "source": "synthetic", "created_at": "2024-01-10T20:00:00", "resolved_at": "2024-01-10T21:00:00", "resolution": "Killed long-running analytics queries and optimized slow query patterns. Added query timeout limits. Replication caught up within 10 minutes.", "labels": ["data", "medium", "database"], "metadata": {"generator": "sample"}}
{"id": "sample-007", "title": "SSL certificate expired on api.example.com", "description": "SSL certificate for api.example.com expired. All HTTPS traffic to API gateway failing. Certificate expired at 00:00 UTC. Monitoring alert was not configured.", "category": "security", "severity": "critical", "source": "synthetic", "created_at": "2024-01-09T00:05:00", "resolved_at": "2024-01-09T00:35:00", "resolution": "Renewed certificate via Let's Encrypt and deployed to API gateway. Implemented certificate expiry monitoring with 30 day warning alerts.", "labels": ["security", "critical", "ssl"], "metadata": {"generator": "sample"}}
{"id": "sample-008", "title": "Stripe API returning intermittent 500 errors", "description": "External dependency Stripe returning 500 errors for 15% of payment requests. Started at 13:00 UTC. Impact: failed checkouts for affected customers. No circuit breaker configured.", "category": "dependency", "severity": "high", "source": "synthetic", "created_at": "2024-01-08T13:00:00", "resolved_at": "2024-01-08T14:30:00", "resolution": "Implemented circuit breaker for Stripe API calls. Added retry logic with exponential backoff. Stripe resolved their issues after 90 minutes.", "labels": ["dependency", "high", "stripe"], "metadata": {"generator": "sample"}}
{"id": "sample-009", "title": "DNS resolution failures for internal services", "description": "Intermittent DNS resolution failures for internal.example.com. 25% of requests timing out. Affects services: user-service, order-service, notification-service. DNS provider: CoreDNS.", "category": "network", "severity": "medium", "source": "synthetic", "created_at": "2024-01-07T15:00:00", "resolved_at": "2024-01-07T16:00:00", "resolution": "Increased CoreDNS replicas from 2 to 4. Added DNS caching at application level. Resolution failures dropped to 0%.", "labels": ["network", "medium", "dns"], "metadata": {"generator": "sample"}}
{"id": "sample-010", "title": "Canary deployment showing high error rate", "description": "Canary deployment of search-api v3.0 detecting 8% error rate on new version. Baseline error rate is 0.5%. Canary weight at 10%. Errors are NullPointerException in SearchController.", "category": "deployment", "severity": "medium", "source": "synthetic", "created_at": "2024-01-06T10:00:00", "resolved_at": "2024-01-06T11:00:00", "resolution": "Halted canary rollout. Identified null pointer in new search ranking code. Fixed null check and redeployed. Canary completed successfully.", "labels": ["deployment", "medium", "canary"], "metadata": {"generator": "sample"}}
